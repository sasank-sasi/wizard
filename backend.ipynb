{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
    "import torch\n",
    "import librosa\n",
    "import numpy as np\n",
    "\n",
    "# Load the Whisper model and processor from Hugging Face\n",
    "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\")\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcript:  The stale smell of old beer lingers. It takes heat to bring out the odor. A cold dip restores health and zest. A salt pickle tastes fine with ham. Tacos al pastor are my favorite. A zestful food is the hot cross bun.\n"
     ]
    }
   ],
   "source": [
    "def transcribe_audio(file_path):\n",
    "    # Load and process the audio file\n",
    "    audio_data, sampling_rate = librosa.load(file_path, sr=16000)\n",
    "    audio_input = processor(audio_data, sampling_rate=sampling_rate, return_tensors=\"pt\", language='en').input_features\n",
    "\n",
    "    # Generate transcription\n",
    "    with torch.no_grad():\n",
    "        generated_ids = model.generate(audio_input)\n",
    "        transcript = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcript\n",
    "\n",
    "# Test the speech-to-text function on an audio file\n",
    "if __name__ == '__main__':\n",
    "    audio_file_path = \"/Users/sasanksasi/Downloads/project/wizard/audio_sample/harvard.wav\"\n",
    "    \n",
    "    # Call the transcription function\n",
    "    transcript = transcribe_audio(audio_file_path)\n",
    "    \n",
    "    print(\"Transcript:\", transcript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-24 15:18:41,962 - INFO - Processing transcript and saving to: /Users/sasanksasi/Downloads/project/wizard/output/transcript_analysis_20250224_151841.json\n",
      "2025-02-24 15:18:41,963 - INFO - Sending request to Groq API\n",
      "2025-02-24 15:18:42,455 - INFO - HTTP Request: POST https://api.groq.com/openai/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-02-24 15:18:42,463 - INFO - Analysis saved to: /Users/sasanksasi/Downloads/project/wizard/output/transcript_analysis_20250224_151841.json\n",
      "2025-02-24 15:18:42,463 - INFO - Analysis completed successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 === Transcript Analysis ===\n",
      "\n",
      "\n",
      "📝 Full Transcript\n",
      "─────────────────\n",
      "The stale smell of old beer lingers. It takes heat to bring out the odor. A cold dip restores health and zest. A salt pickle tastes fine with ham. Tacos al pastor are my favorite. A zestful food is the hot cross bun.\n",
      "\n",
      "📌 Summary\n",
      "─────────\n",
      "The speaker discusses the effects of heat on odors and their favorite foods.\n",
      "\n",
      "🔑 Key Points\n",
      "────────────\n",
      "• heat brings out odors\n",
      "• cold dip restores health\n",
      "\n",
      "📅 Dates Mentioned\n",
      "─────────────────\n",
      "None found\n",
      "\n",
      "📧 Email Addresses\n",
      "─────────────────\n",
      "None found\n",
      "\n",
      "✅ Action Items\n",
      "──────────────\n",
      "None found\n",
      "\n",
      "💾 Analysis saved to: /Users/sasanksasi/Downloads/project/wizard/output/transcript_analysis_20250224_151841.json\n",
      "\n",
      "✅ Analysis completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import re\n",
    "from typing import Dict, Any, Optional, List\n",
    "from datetime import datetime\n",
    "from groq import Groq\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler(),\n",
    "        logging.FileHandler('transcript_analysis.log')\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize Groq API client\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "if not api_key:\n",
    "    raise EnvironmentError(\"GROQ_API_KEY not found in environment variables\")\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    \"\"\"Clean and normalize text\"\"\"\n",
    "    return text.strip() if text else \"\"\n",
    "\n",
    "def extract_dates(text: str) -> List[str]:\n",
    "    \"\"\"Extract dates from text using regex\"\"\"\n",
    "    date_pattern = r'\\b(?:\\d{1,2}[-/]\\d{1,2}[-/]\\d{2,4}|(?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]* \\d{1,2}(?:st|nd|rd|th)?,? \\d{4})\\b'\n",
    "    return list(set(re.findall(date_pattern, text, re.IGNORECASE)))\n",
    "\n",
    "def extract_emails(text: str) -> List[str]:\n",
    "    \"\"\"Extract email addresses from text\"\"\"\n",
    "    email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b'\n",
    "    return list(set(re.findall(email_pattern, text)))\n",
    "\n",
    "def analyze_transcript_with_groq(transcript: str) -> Dict[str, Any]:\n",
    "    \"\"\"Analyze transcript using Groq API with improved error handling\"\"\"\n",
    "    if not transcript:\n",
    "        return {\"error\": \"Empty transcript provided\"}\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this transcript and provide only a JSON response with this structure:\n",
    "    {{\n",
    "        \"transcript\": \"full transcript text\",\n",
    "        \"summary\": \"summary of the transcript\",\n",
    "        \"key_points\": [\"point 1\", \"point 2, ...\"],\n",
    "        \"dates\": [\"date/time reference 1\", \"date/time reference 2, ...\"],\n",
    "        \"emails\": [\"email1@domain.com\"],\n",
    "        \"action_items\": [\"action 1\", \"action 2, ...\"]\n",
    "    }}\n",
    "    \n",
    "    Transcript: {transcript}\n",
    "    \n",
    "    Important: Return ONLY valid JSON, no markdown or additional text.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        logging.info(\"Sending request to Groq API\")\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are a JSON-only response bot. Always respond with valid JSON matching the specified structure. No markdown, no explanations, just JSON.\"\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ],\n",
    "            model=\"llama3-8b-8192\",\n",
    "            temperature=0.1\n",
    "        )\n",
    "        \n",
    "        # Clean and parse response\n",
    "        response = chat_completion.choices[0].message.content.strip()\n",
    "        response = re.sub(r'^[^{]*', '', response)  # Remove anything before first {\n",
    "        response = re.sub(r'[^}]*$', '', response)  # Remove anything after last }\n",
    "        \n",
    "        try:\n",
    "            parsed_response = json.loads(response)\n",
    "            \n",
    "            # Add extracted dates and emails\n",
    "            extracted_dates = extract_dates(transcript)\n",
    "            extracted_emails = extract_emails(transcript)\n",
    "            \n",
    "            if extracted_dates:\n",
    "                parsed_response[\"dates\"] = list(set(parsed_response.get(\"dates\", []) + extracted_dates))\n",
    "            if extracted_emails:\n",
    "                parsed_response[\"emails\"] = list(set(parsed_response.get(\"emails\", []) + extracted_emails))\n",
    "            \n",
    "            return parsed_response\n",
    "            \n",
    "        except json.JSONDecodeError as e:\n",
    "            logging.error(f\"JSON parsing error: {str(e)}\")\n",
    "            return {\n",
    "                \"error\": \"JSON parsing error\",\n",
    "                \"details\": str(e),\n",
    "                \"raw_response\": response\n",
    "            }\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Groq API error: {str(e)}\")\n",
    "        return {\"error\": f\"API error: {str(e)}\"}\n",
    "\n",
    "def validate_response(response: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Validate response structure and types\"\"\"\n",
    "    if \"error\" in response:\n",
    "        return False\n",
    "        \n",
    "    required_fields = {\n",
    "        \"transcript\": str,\n",
    "        \"summary\": str,\n",
    "        \"key_points\": list,\n",
    "        \"dates\": list,\n",
    "        \"emails\": list,\n",
    "        \"action_items\": list\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        for field, field_type in required_fields.items():\n",
    "            if field not in response or not isinstance(response[field], field_type):\n",
    "                return False\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def format_and_print_analysis(analysis: Dict[str, Any]) -> None:\n",
    "    \"\"\"Format and print analysis results\"\"\"\n",
    "    if \"error\" in analysis:\n",
    "        print(\"\\n❌ Error in Analysis:\")\n",
    "        print(f\"   {analysis['error']}\")\n",
    "        if \"raw_response\" in analysis:\n",
    "            print(\"\\nRaw Response:\")\n",
    "            print(analysis['raw_response'])\n",
    "        return\n",
    "\n",
    "    print(\"\\n📊 === Transcript Analysis ===\\n\")\n",
    "    \n",
    "    sections = {\n",
    "        \"transcript\": \"📝 Full Transcript\",\n",
    "        \"summary\": \"📌 Summary\",\n",
    "        \"key_points\": \"🔑 Key Points\",\n",
    "        \"dates\": \"📅 Dates Mentioned\",\n",
    "        \"emails\": \"📧 Email Addresses\",\n",
    "        \"action_items\": \"✅ Action Items\"\n",
    "    }\n",
    "    \n",
    "    for key, title in sections.items():\n",
    "        print(f\"\\n{title}\")\n",
    "        print(\"─\" * len(title))\n",
    "        value = analysis.get(key, [])\n",
    "        if isinstance(value, list):\n",
    "            if value:\n",
    "                for item in value:\n",
    "                    print(f\"• {item}\")\n",
    "            else:\n",
    "                print(\"None found\")\n",
    "        else:\n",
    "            print(value if value else \"None provided\")\n",
    "\n",
    "def save_analysis_to_file(analysis: Dict[str, Any], output_path: str) -> None:\n",
    "    \"\"\"Save analysis results to JSON file\"\"\"\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(analysis, f, indent=2, ensure_ascii=False)\n",
    "        logging.info(f\"Analysis saved to: {output_path}\")\n",
    "        print(f\"\\n💾 Analysis saved to: {output_path}\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving analysis: {str(e)}\")\n",
    "        print(f\"\\n❌ Error saving analysis: {str(e)}\")\n",
    "\n",
    "def process_transcript(transcript_text: str, output_file: Optional[str] = None) -> Dict[str, Any]:\n",
    "    \"\"\"Process transcript and save results\"\"\"\n",
    "    try:\n",
    "        # Clean input\n",
    "        transcript_text = clean_text(transcript_text)\n",
    "        if not transcript_text:\n",
    "            return {\"error\": \"Empty transcript provided\"}\n",
    "            \n",
    "        # Get analysis\n",
    "        analysis = analyze_transcript_with_groq(transcript_text)\n",
    "        \n",
    "        # Validate response\n",
    "        if not validate_response(analysis):\n",
    "            return {\"error\": \"Invalid response structure\", \"raw_response\": analysis}\n",
    "        \n",
    "        # Print analysis\n",
    "        format_and_print_analysis(analysis)\n",
    "        \n",
    "        # Save if path provided\n",
    "        if output_file:\n",
    "            save_analysis_to_file(analysis, output_file)\n",
    "        \n",
    "        return analysis\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error processing transcript: {str(e)}\")\n",
    "        return {\"error\": f\"Processing error: {str(e)}\"}\n",
    "\n",
    "# Update the main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    # Test transcript\n",
    "    # test_transcript = \"\"\"\n",
    "    # Hello everyone, welcome to our weekly meeting on March 15th, 2024.\n",
    "    \n",
    "    # Our main action items from last week:\n",
    "    # 1. John needs to send the report to sarah.smith@company.com\n",
    "    # 2. Meeting with clients scheduled for March 20th at 2 PM\n",
    "    # 3. Project deadline is set for April 1st, 2024\n",
    "    \n",
    "    # Please contact support@company.com for any technical issues.\n",
    "    # Next meeting is scheduled for March 22nd, 2024.\n",
    "    # \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Create output directory in current working directory\n",
    "        output_dir = os.path.join(os.getcwd(), \"output\")\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Generate output path with timestamp\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        output_path = os.path.join(\n",
    "            output_dir,\n",
    "            f\"transcript_analysis_{timestamp}.json\"\n",
    "        )\n",
    "        \n",
    "        logging.info(f\"Processing transcript and saving to: {output_path}\")\n",
    "        results = process_transcript(transcript, output_path)\n",
    "        \n",
    "        # Print status\n",
    "        if \"error\" not in results:\n",
    "            print(\"\\n✅ Analysis completed successfully!\")\n",
    "            logging.info(\"Analysis completed successfully\")\n",
    "        else:\n",
    "            print(f\"\\n❌ Analysis failed: {results['error']}\")\n",
    "            logging.error(f\"Analysis failed: {results['error']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logging.error(f\"Fatal error: {str(e)}\")\n",
    "        print(f\"\\n❌ Fatal error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
